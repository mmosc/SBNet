{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.110227Z",
     "start_time": "2024-07-27T16:22:09.683820Z"
    }
   },
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import sys\n",
    "sys.path.append(\"/home/marta/jku/SBNet/ssnet_fop\")\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from retrieval_model import FOP\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.136472Z",
     "start_time": "2024-07-27T16:22:11.112531Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "4184629c9d49e195",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "df4d5a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.142996Z",
     "start_time": "2024-07-27T16:22:11.139484Z"
    }
   },
   "source": "data_folder = '/home/marta/jku/LLaVA/mmimdb'",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "fd8d2c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.151670Z",
     "start_time": "2024-07-27T16:22:11.145451Z"
    }
   },
   "source": [
    "texts_folder = os.path.join(data_folder, 'llava_encoded_texts')\n",
    "\n",
    "train_text_df = os.path.join(texts_folder, 'llava_plot_first_latent_train.csv')\n",
    "test_text_df = os.path.join(texts_folder, 'llava_plot_first_latent_test.csv')\n",
    "\n",
    "images_folder = os.path.join(data_folder, 'llava_encoded_images')\n",
    "\n",
    "train_image_df = os.path.join(images_folder, 'llava_images_latent_train.csv')\n",
    "test_image_df = os.path.join(images_folder, 'llava_images_latent_test.csv')\n",
    "\n",
    "labels = ['action', 'adult', 'adventure', 'animation', 'biography', 'comedy',\n",
    "       'crime', 'documentary', 'drama', 'family', 'fantasy', 'film-noir',\n",
    "       'history', 'horror', 'music', 'musical', 'mystery', 'news',\n",
    "       'reality-tv', 'romance', 'sci-fi', 'short', 'sport', 'talk-show',\n",
    "       'thriller', 'war', 'western']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "bf6e6bbe31f62c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.228769Z",
     "start_time": "2024-07-27T16:22:11.158311Z"
    }
   },
   "source": [
    "def read_data(FLAGS):\n",
    "\n",
    "    print('Split Type: %s'%(FLAGS.split_type))\n",
    "\n",
    "    if FLAGS.split_type == 'text_only':\n",
    "        print('Reading Text Train')\n",
    "        train_file_text = train_text_df\n",
    "        train_data = pd.read_csv(train_file_text, index_col='item_id')\n",
    "        train_label = train_data[labels]\n",
    "        train_data = train_data.drop(columns=labels)\n",
    "        train_data = np.asarray(train_data)\n",
    "        # Shuffle the data also if only one modality is used\n",
    "        # combined = list(zip(train_data, train_label))\n",
    "        # random.shuffle(combined)\n",
    "        # train_data, train_label = zip(*combined)\n",
    "\n",
    "        return train_data, train_label\n",
    "\n",
    "    elif FLAGS.split_type == 'image_only':\n",
    "        print('Reading Image Train')\n",
    "        train_file_image = train_image_df\n",
    "        train_data = pd.read_csv(train_file_image, index_col='item_id')\n",
    "        train_label = train_data[labels]\n",
    "        train_data = train_data.drop(columns=labels)\n",
    "        train_data = np.asarray(train_data)\n",
    "        # Shuffle the data also if only one modality is used\n",
    "        # combined = list(zip(train_data, train_label))\n",
    "        # random.shuffle(combined)\n",
    "        # train_data, train_label = zip(*combined)\n",
    "\n",
    "        return train_data, train_label\n",
    "\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "\n",
    "    train_file_face = '/share/hel/datasets/voxceleb/sbnet_feats/data/face/facenetfaceTrain.csv'\n",
    "    train_file_voice = '/share/hel/datasets/voxceleb/sbnet_feats/data/voice/voiceTrain.csv'\n",
    "\n",
    "    print('Reading Train Faces')\n",
    "    img_train = pd.read_csv(train_file_face, header=None)\n",
    "    train_tmp = img_train[512]\n",
    "    img_train = np.asarray(img_train)\n",
    "    img_train = img_train[:, :-1]\n",
    "\n",
    "    train_tmp = np.asarray(train_tmp)\n",
    "    train_tmp = train_tmp.reshape((train_tmp.shape[0], 1))\n",
    "    print('Reading Train Voices')\n",
    "    voice_train = pd.read_csv(train_file_voice, header=None)\n",
    "    voice_train = np.asarray(voice_train)\n",
    "    voice_train = voice_train[:, :-1]\n",
    "\n",
    "    combined = list(zip(img_train, voice_train, train_tmp))\n",
    "    # todo marta: why do we need to shuffle here?\n",
    "    random.shuffle(combined)\n",
    "    img_train, voice_train, train_tmp = zip(*combined)\n",
    "\n",
    "    if FLAGS.split_type == 'random':\n",
    "        # todo marta: aren't we doubling the dataset, like this?\n",
    "        train_data = np.vstack((img_train, voice_train))\n",
    "        train_label = np.vstack((train_tmp, train_tmp))\n",
    "        combined = list(zip(train_data, train_label))\n",
    "        random.shuffle(combined)\n",
    "        train_data, train_label = zip(*combined)\n",
    "        train_data = np.asarray(train_data).astype(np.float)\n",
    "        train_label = np.asarray(train_label)\n",
    "\n",
    "    elif FLAGS.split_type == 'vfvf':\n",
    "        for i in range(len(voice_train)):\n",
    "            train_data.append(voice_train[i])\n",
    "            train_data.append(img_train[i])\n",
    "            train_label.append(train_tmp[i])\n",
    "            train_label.append(train_tmp[i])\n",
    "\n",
    "    elif FLAGS.split_type == 'fvfv':\n",
    "        for i in range(len(voice_train)):\n",
    "            train_data.append(img_train[i])\n",
    "            train_data.append(voice_train[i])\n",
    "            train_label.append(train_tmp[i])\n",
    "            train_label.append(train_tmp[i])\n",
    "\n",
    "    elif FLAGS.split_type == 'hefhev':\n",
    "        train_data = np.vstack((img_train, voice_train))\n",
    "        train_label = np.vstack((train_tmp, train_tmp))\n",
    "\n",
    "    elif FLAGS.split_type == 'hevhef':\n",
    "        train_data = np.vstack((voice_train, img_train))\n",
    "        train_label = np.vstack((train_tmp, train_tmp))\n",
    "\n",
    "    else:\n",
    "        print('Invalid Split Type')\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(train_label)\n",
    "    train_label = le.transform(train_label)\n",
    "\n",
    "    # print(\"Train file length\", len(img_train))\n",
    "    # print('Shuffling\\n')\n",
    "\n",
    "    train_data = np.asarray(train_data).astype(np.float)\n",
    "    train_label = np.asarray(train_label)\n",
    "\n",
    "    return train_data, train_label\n",
    "\n",
    "def get_batch(batch_index, batch_size, labels, f_lst):\n",
    "    start_ind = batch_index * batch_size\n",
    "    end_ind = (batch_index + 1) * batch_size\n",
    "    return np.asarray(f_lst[start_ind:end_ind]), np.asarray(labels[start_ind:end_ind])\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def main(train_data, train_label):\n",
    "    \"\"\"\n",
    "    train_data.shape = (num_train_instances, embedding_size)\n",
    "    train_label.shape = (num_train_instances, num_labels)\n",
    "    \"\"\"\n",
    "    n_class = train_label.shape[1]\n",
    "    train_label_np = np.asarray(train_label)\n",
    "    model = FOP(FLAGS, train_data.shape[1], n_class)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    # ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "    bce_logits_loss = nn.BCEWithLogitsLoss().cuda()\n",
    "    # We do not necessarily want orthogonal projection loss imo\n",
    "    # opl_loss = OrthogonalProjectionLoss().cuda()\n",
    "    # TODO adapt code to remove opl\n",
    "    opl_loss = None\n",
    "    \n",
    "    if FLAGS.cuda:\n",
    "        model.cuda()\n",
    "        # ce_loss.cuda()    \n",
    "        bce_logits_loss.cuda()\n",
    "        if opl_loss:\n",
    "            opl_loss.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=FLAGS.lr, weight_decay=0.01)\n",
    "\n",
    "    n_parameters = sum([p.data.nelement() for p in model.parameters()])\n",
    "    print('  + Number of params: {}'.format(n_parameters))\n",
    "    \n",
    "    \n",
    "    for alpha in FLAGS.alpha_list:\n",
    "        epoch = 1\n",
    "        # todo check: why len and not train_label[0] ? \n",
    "        num_of_batches = (len(train_label) // FLAGS.batch_size)\n",
    "        loss_plot = []\n",
    "        train_precision_list = []\n",
    "        train_recall_list = []\n",
    "        train_f1_list = []\n",
    "        \n",
    "        loss_per_epoch = 0.\n",
    "        # todo adapt code to remove s_fac and d_fac\n",
    "        s_fac_per_epoch = 0.\n",
    "        d_fac_per_epoch = 0.\n",
    "        txt_dir = 'output'\n",
    "        save_dir = 'fc2_%s_%s_alpha_%0.2f'%(FLAGS.split_type, FLAGS.save_dir, alpha)\n",
    "        txt = '%s/ce_opl_%03d_%0.2f.txt'%(txt_dir, FLAGS.max_num_epoch, alpha)\n",
    "        \n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        if not os.path.exists(txt_dir):\n",
    "            os.makedirs(txt_dir)\n",
    "        \n",
    "        with open(txt,'w+') as f:\n",
    "            f.write('EPOCH\\tLOSS\\tprecision\\trecall\\tS_FAC\\tD_FAC\\n')\n",
    "        \n",
    "        save_best = 'best_%s'%(save_dir)\n",
    "        \n",
    "        if not os.path.exists(save_best):\n",
    "            os.mkdir(save_best)\n",
    "        with open(txt,'a+') as f:\n",
    "            while (epoch < FLAGS.max_num_epoch):\n",
    "                print('%s\\tEpoch %03d'%(FLAGS.split_type, epoch))\n",
    "                for idx in tqdm(range(num_of_batches)):\n",
    "                    train_batch, batch_labels = get_batch(idx, FLAGS.batch_size, train_label, train_data)\n",
    "                    # voice_feats, _ = get_batch(idx, FLAGS.batch_size, train_label, voice_train)\n",
    "                    loss_tmp, loss_opl, loss_soft, s_fac, d_fac = train(train_batch, \n",
    "                                                                 batch_labels, \n",
    "                                                                 model, optimizer, bce_logits_loss, opl_loss, alpha)\n",
    "                    loss_per_epoch += loss_tmp\n",
    "                    s_fac_per_epoch += s_fac\n",
    "                    d_fac_per_epoch += d_fac\n",
    "                \n",
    "                # todo check: should we really divide by the number of batches? this leads to an incorrect loss \n",
    "                #  if the last batch is smaller.\n",
    "                loss_per_epoch /= num_of_batches\n",
    "                s_fac_per_epoch /= num_of_batches\n",
    "                d_fac_per_epoch /= num_of_batches\n",
    "                \n",
    "                loss_plot.append(loss_per_epoch)\n",
    "                # print(train_data, train_label)\n",
    "                precision, recall, f1 = eval(train_data, train_label_np, model)\n",
    "                train_precision_list.append(precision)\n",
    "                train_recall_list.append(recall)\n",
    "                train_f1_list.append(f1)\n",
    "                \n",
    "                # ToDo\n",
    "                # precision, recall, f1 = eval(, , model)\n",
    "                # test_precision_list += [precision]\n",
    "                # test_recall_list += [recall]\n",
    "                # test_f1_list += [f1]\n",
    "                \n",
    "                save_checkpoint({\n",
    "                   'epoch': epoch,\n",
    "                   'state_dict': model.state_dict()}, save_dir, 'checkpoint_%04d_%0.3f.pth.tar'%(epoch, f1*100))\n",
    "\n",
    "                # print('==> Epoch: %d/%d Loss: %0.2f Alpha:%0.2f, Min_train_f1: %0.2f'%(epoch, FLAGS.max_num_epoch, loss_per_epoch, alpha, min(train_f1_list)))\n",
    "#                 if eer <= min(eer_list):\n",
    "#                     min_eer = eer\n",
    "#                     max_auc = auc\n",
    "#                     save_checkpoint({\n",
    "#                     'epoch': epoch,\n",
    "#                     'state_dict': model.state_dict()}, save_best, 'checkpoint.pth.tar')\n",
    "                # ToDo \n",
    "                # eer, auc = 0., 0.\n",
    "                # f.write('%04d\\t%0.4f\\t%0.2f\\t%0.2f\\t%0.2f\\t%0.2f\\n'%(epoch, loss_per_epoch, eer, auc, s_fac_per_epoch, d_fac_per_epoch))\n",
    "                # loss_per_epoch = 0\n",
    "                # s_fac_per_epoch = 0\n",
    "                # d_fac_per_epoch = 0\n",
    "                # epoch += 1\n",
    "        \n",
    "        return loss_plot, train_f1_list                \n",
    "#         return loss_plot, min_eer, max_auc\n",
    "\n",
    "\n",
    "def eval(train_batch, labels, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_batch = torch.from_numpy(train_batch).float()\n",
    "        \n",
    "        if FLAGS.cuda:\n",
    "            train_batch = train_batch.cuda()\n",
    "        train_batch = Variable(train_batch)\n",
    "        comb = model.train_forward(train_batch)\n",
    "        \n",
    "        predictions = torch.sigmoid(comb[1]).cpu().numpy()\n",
    "        predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "        \n",
    "        precision = precision_score(labels, predictions, average='samples')\n",
    "        recall = recall_score(labels, predictions, average='samples')\n",
    "        f1 = f1_score(labels, predictions, average='samples')\n",
    "        \n",
    "        return precision, recall, f1\n",
    "    \n",
    "    \n",
    "def train(train_batch, labels, model, optimizer, bce_logits_loss, opl_loss, alpha):\n",
    "    \n",
    "    average_loss = RunningAverage()\n",
    "    soft_losses = RunningAverage()\n",
    "    if opl_loss:\n",
    "        opl_losses = RunningAverage()\n",
    "\n",
    "    model.train()\n",
    "    # face_feats = torch.from_numpy(face_feats).float()\n",
    "    train_batch = torch.from_numpy(train_batch).float()\n",
    "    labels = torch.from_numpy(labels).float()\n",
    "    \n",
    "    if FLAGS.cuda:\n",
    "        train_batch, labels = train_batch.cuda(), labels.cuda()\n",
    "\n",
    "    train_batch, labels = Variable(train_batch), Variable(labels)\n",
    "    comb = model.train_forward(train_batch)\n",
    "    \n",
    "    # loss_soft = ce_loss(comb[1], labels)\n",
    "    loss_soft = bce_logits_loss(comb[1], labels)    \n",
    "    \n",
    "    if opl_loss:\n",
    "        loss_opl, s_fac, d_fac = opl_loss(comb[0], labels)\n",
    "        loss = loss_soft + alpha * loss_opl\n",
    "    else: \n",
    "        loss = loss_soft\n",
    "        s_fac, d_fac = 0., 0.\n",
    "        opl_losses = 0.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    average_loss.update(loss.item())\n",
    "    if opl_loss:\n",
    "        opl_losses.update(loss_opl.item())\n",
    "    soft_losses.update(loss_soft.item())\n",
    "    \n",
    "    optimizer.step()\n",
    "    if opl_loss:\n",
    "        return average_loss.avg(), opl_losses.avg(), soft_losses.avg(), s_fac, d_fac\n",
    "    else:\n",
    "        return average_loss.avg(), opl_losses, soft_losses.avg(), s_fac, d_fac\n",
    "\n",
    "class RunningAverage(object):\n",
    "    def __init__(self):\n",
    "        self.value_sum = 0.\n",
    "        self.num_items = 0. \n",
    "\n",
    "    def update(self, val):\n",
    "        self.value_sum += val \n",
    "        self.num_items += 1\n",
    "\n",
    "    def avg(self):\n",
    "        average = 0.\n",
    "        if self.num_items > 0:\n",
    "            average = self.value_sum / self.num_items\n",
    "\n",
    "        return average\n",
    " \n",
    "def save_checkpoint(state, directory, filename):\n",
    "    filename = os.path.join(directory, filename)\n",
    "    torch.save(state, filename)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "7d9e30cf6e04ffbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.243243Z",
     "start_time": "2024-07-27T16:22:11.231420Z"
    }
   },
   "source": [
    "global FLAGS"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "d40ec09aa487851f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:11.268516Z",
     "start_time": "2024-07-27T16:22:11.245169Z"
    }
   },
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S', help='Random Seed')\n",
    "parser.add_argument('--cuda', action='store_true', default=True, help='CUDA Training')\n",
    "parser.add_argument('--save_dir', type=str, default='model', help='Directory for saving checkpoints.')\n",
    "parser.add_argument('--lr', type=float, default=1e-2, metavar='LR',\n",
    "                    help='learning rate (default: 1e-4)') \n",
    "parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training.')\n",
    "parser.add_argument('--max_num_epoch', type=int, default=100, help='Max number of epochs to train, number')\n",
    "parser.add_argument('--alpha_list', type=list, default=[1], help='Alpha Values List')\n",
    "parser.add_argument('--dim_embed', type=int, default=64,\n",
    "                    help='Embedding Size')\n",
    "parser.add_argument('--split_type', type=str, default='image_only', help='split_type')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--split_type'], dest='split_type', nargs=None, const=None, default='image_only', type=<class 'str'>, choices=None, help='split_type', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "47028af40cb5fd17",
   "metadata": {},
   "source": [
    "FLAGS, unparsed = parser.parse_known_args()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e84b804a85f8343b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:47.411692Z",
     "start_time": "2024-07-27T16:22:11.282913Z"
    }
   },
   "source": [
    "train_data, train_label = read_data(FLAGS)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Type: image_only\n",
      "Reading Image Train\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "fd90c488685380b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:47.424642Z",
     "start_time": "2024-07-27T16:22:47.413535Z"
    }
   },
   "source": [
    "print('Split Type: %s'%(FLAGS.split_type))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Type: image_only\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "fec7e50cb0caa13c",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:53.604388Z",
     "start_time": "2024-07-27T16:22:47.426642Z"
    }
   },
   "source": "losses, f1_scores = main(train_data, train_label)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marta/miniforge3/envs/sbnet/lib/python3.6/site-packages/ipykernel_launcher.py:111: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Number of params: 464987\n",
      "image_only\tEpoch 001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:02<00:00, 46.71it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [15552, 419904]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-0230034e3f7f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlosses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf1_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-5-e7b2a606650b>\u001B[0m in \u001B[0;36mmain\u001B[0;34m(train_data, train_label)\u001B[0m\n\u001B[1;32m    194\u001B[0m                 \u001B[0mloss_plot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss_per_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m                 \u001B[0;31m# print(train_data, train_label)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m                 \u001B[0mprecision\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecall\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_label_np\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m                 \u001B[0mtrain_precision_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprecision\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m                 \u001B[0mtrain_recall_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecall\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-e7b2a606650b>\u001B[0m in \u001B[0;36meval\u001B[0;34m(train_batch, labels, model)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 243\u001B[0;31m         \u001B[0mprecision\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprecision_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'samples'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    244\u001B[0m         \u001B[0mrecall\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrecall_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'samples'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    245\u001B[0m         \u001B[0mf1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'samples'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36mprecision_score\u001B[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1660\u001B[0m                                                  \u001B[0mwarn_for\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'precision'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1661\u001B[0m                                                  \u001B[0msample_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1662\u001B[0;31m                                                  zero_division=zero_division)\n\u001B[0m\u001B[1;32m   1663\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1664\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36mprecision_recall_fscore_support\u001B[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1463\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"beta should be >=0 in the F-beta score\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1464\u001B[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001B[0;32m-> 1465\u001B[0;31m                                     pos_label)\n\u001B[0m\u001B[1;32m   1466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1467\u001B[0m     \u001B[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36m_check_set_wise_labels\u001B[0;34m(y_true, y_pred, average, labels, pos_label)\u001B[0m\n\u001B[1;32m   1275\u001B[0m                          str(average_options))\n\u001B[1;32m   1276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1277\u001B[0;31m     \u001B[0my_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1278\u001B[0m     \u001B[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1279\u001B[0m     \u001B[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0my_pred\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0marray\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mindicator\u001B[0m \u001B[0mmatrix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m     \"\"\"\n\u001B[0;32m---> 83\u001B[0;31m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m     \u001B[0mtype_true\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[0mtype_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype_of_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniforge3/envs/sbnet/lib/python3.6/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    318\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0muniques\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    319\u001B[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001B[0;32m--> 320\u001B[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001B[0m\u001B[1;32m    321\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [15552, 419904]"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T16:22:53.607059Z",
     "start_time": "2024-07-27T16:22:53.606892Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "83678eff1a9ad0fe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
